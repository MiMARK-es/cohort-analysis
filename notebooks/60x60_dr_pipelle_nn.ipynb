{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort 60x60 Neural Networks AUCs analysis\n",
    "\n",
    "### Imports and environment setup\n",
    "\n",
    "- Date of run: 2024-12-30\n",
    "- Environment: python 3.12\n",
    "- Packages required: pandas, numpy, sklearn, statsmodels, seaborn, matplotlib, tensorflow, keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include in the environment the code directory with the utils function\n",
    "import sys\n",
    "sys.path.append('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 08:53:16.032404: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-24 08:53:16.096556: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-24 08:53:16.146549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737708796.196450   12467 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737708796.212776   12467 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 08:53:16.325105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Utils imports\n",
    "import cohort_analysis_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Remove cell printing limits\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded dataset\n",
    "file_path_mmk = '../data/60x60_dr_mmk_20241209.csv'\n",
    "data_mmk = pd.read_csv(file_path_mmk, delimiter='\\t', index_col=0)\n",
    "\n",
    "# Rename column with trailing space\n",
    "data_mmk.rename(columns={'KPYM ng/mL ': 'KPYM ng/mL'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipelle = pd.read_csv('../data/pipelle_results_20250121.csv' , sep='\\t', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_120, merge the columns from df_pipelle named 'Result', using the index to match the rows, \n",
    "# make the \"Material Insuficiente o ausencia de diagnóstico\" to have a value of 2\n",
    "# make the NaN values to have a value of 1\n",
    "# make the \"Cáncer\" to have a value of 8\n",
    "# and all the rest to have a value of 4\n",
    "#remove duplicate indexes in df_pipelle\n",
    "df_pipelle = df_pipelle[~df_pipelle.index.duplicated(keep='first')]\n",
    "data_mmk['Result'] = data_mmk.index.map(df_pipelle['Result'])\n",
    "data_mmk['Result'] = data_mmk['Result'].replace({\"Material Insuficiente o ausencia de diagnóstico\": 1, \"Cáncer\": 2})\n",
    "data_mmk['Result'] = data_mmk['Result'].fillna(1)\n",
    "# Now all the remaining string values are replaced by 4\n",
    "data_mmk['Result'] = data_mmk['Result'].replace({value: 0 for value in data_mmk['Result'].unique() if type(value) == str})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Collection center', 'Age', 'Pathology', 'Hystology grade',\n",
       "       'Hystology type', 'FIGO stage 2009', 'TCGA',\n",
       "       'Time between collection and processing (h)', 'Group time',\n",
       "       'Collected volume (mL)', 'Sample visual description', 'Hemolysis', 'pH',\n",
       "       'Collected at', 'MMP9 ng/mL', 'HSPB1 ng/mL', 'PERM ng/mL',\n",
       "       'ADIPOQ ng/mL', 'TIMP-2 ng/mL', 'AGRIN ng/mL', 'KPYM ng/mL',\n",
       "       'Total protein BCA mg/mL', 'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mmk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename column with trailing space\n",
    "data_mmk.rename(columns={'KPYM ng/mL ': 'KPYM ng/mL'}, inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "features = ['MMP9 ng/mL', 'HSPB1 ng/mL', 'PERM ng/mL', 'ADIPOQ ng/mL', 'TIMP-2 ng/mL', 'AGRIN ng/mL', 'KPYM ng/mL', \"Result\"]\n",
    "target = 'Pathology'\n",
    "\n",
    "# Preprocess data\n",
    "data_mmk[features] = data_mmk[features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "data_clean = data_mmk[features + [target]].dropna()\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "data_clean[target] = label_encoder.fit_transform(data_clean[target])\n",
    "\n",
    "# Split into features and target\n",
    "X = data_clean[features]\n",
    "y = data_clean[target]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, nodes=[], dropout_rate=0.5):\n",
    "    # Define the layers of the neural network based on the input parameters\n",
    "    layers= [Dense(nodes[0], input_dim=input_dim, activation='relu')]\n",
    "    layers.append(Dropout(dropout_rate))\n",
    "    for node in nodes[1:]:\n",
    "        layers.append(Dense(node, activation='relu'))\n",
    "        layers.append(Dropout(dropout_rate))\n",
    "    layers.append(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Build the neural network model\n",
    "    model = Sequential(layers)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, X_test, y_test, epochs=100, batch_size=32, verbose=0):\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "                        X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=verbose\n",
    "                        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an overfitted model over the BMKs readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 08:53:18.736869: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-24 08:53:18.736900: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: lradusky\n",
      "2025-01-24 08:53:18.736906: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: lradusky\n",
      "2025-01-24 08:53:18.736988: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2025-01-24 08:53:18.737011: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 470.223.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Neural Network AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(input_dim=X_train.shape[1], nodes=[128, 64, 32], dropout_rate=0.0)\n",
    "\n",
    "# replace the code above with the train_model function\n",
    "history = fit_model(model, X_train, y_train, X_test, y_test, epochs=400, batch_size=32)\n",
    "\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_nn = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_nn = roc_auc_score(y_test, y_pred_proba_nn)\n",
    "\n",
    "print(f'Neural Network AUC: {auc_nn:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Biomarkers Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the biomarkers to use for ratios\n",
    "biomarkers = ['MMP9 ng/mL', 'AGRIN ng/mL', 'TIMP-2 ng/mL', 'PERM ng/mL', 'KPYM ng/mL', 'ADIPOQ ng/mL', 'HSPB1 ng/mL']\n",
    "\n",
    "# Create all possible ratios\n",
    "ratios = []\n",
    "for biomarker1, biomarker2 in itertools.combinations(biomarkers, 2):\n",
    "    ratio_name = f\"{biomarker1}_to_{biomarker2}\"\n",
    "    data_clean[ratio_name] = np.where(data_clean[biomarker2] != 0, \n",
    "                                      data_clean[biomarker1] / data_clean[biomarker2], \n",
    "                                      np.nan)\n",
    "    ratios.append(ratio_name)\n",
    "\n",
    "# Drop rows with NaN values resulting from invalid ratios\n",
    "data_ratios = data_clean.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X_ratios = data_ratios[ratios + [\"Result\"]]\n",
    "y_ratios = data_ratios[target]\n",
    "\n",
    "# Normalize the features\n",
    "X_scaled_ratios = scaler.fit_transform(X_ratios)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train_ratios, X_test_ratios, y_train_ratios, y_test_ratios = train_test_split(\n",
    "    X_scaled_ratios, y_ratios, test_size=0.1, random_state=42, stratify=y_ratios\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, nodes, dropout_rate=0.5, epochs=100, batch_size=16, n_splits=10):\n",
    "    # Convert `y` to a NumPy array for compatibility with KFold\n",
    "    y_array = y.to_numpy()\n",
    "\n",
    "    # Initialize k-fold cross-validator\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_auc_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_idx, val_idx in kfold.split(X, y_array):\n",
    "        # Split the data into training and validation sets\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y_array[train_idx], y_array[val_idx]\n",
    "\n",
    "        # Build the neural network model\n",
    "        model = build_model(input_dim=X_train_fold.shape[1], nodes=nodes, dropout_rate=dropout_rate)\n",
    "\n",
    "        # Train the model\n",
    "        history = fit_model(model, X_train_fold, y_train_fold, X_val_fold, y_val_fold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict probabilities for the validation fold\n",
    "        y_val_pred = model.predict(X_val_fold).flatten()\n",
    "\n",
    "        # Calculate AUC for the current fold\n",
    "        fold_auc = roc_auc_score(y_val_fold, y_val_pred)\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        print(f\"AUC for Fold: {fold_auc}\")\n",
    "\n",
    "    # Compute the average AUC across all folds\n",
    "    average_auc = np.mean(fold_auc_scores)\n",
    "    print(f\"Average AUC across all folds: {average_auc}\")\n",
    "\n",
    "    return average_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the same just with MMP9, AGRIN and TIMP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the biomarkers to use for ratios\n",
    "biomarkers = ['AGRIN ng/mL', 'TIMP-2 ng/mL']\n",
    "\n",
    "# Create all possible ratios\n",
    "ratios = []\n",
    "for biomarker1, biomarker2 in itertools.combinations(biomarkers, 2):\n",
    "    ratio_name = f\"{biomarker1}_to_{biomarker2}\"\n",
    "    data_clean[ratio_name] = np.where(data_clean[biomarker2] != 0, \n",
    "                                      data_clean[biomarker1] / data_clean[biomarker2], \n",
    "                                      np.nan)\n",
    "    ratios.append(ratio_name)\n",
    "\n",
    "# Drop rows with NaN values resulting from invalid ratios\n",
    "data_ratios = data_clean.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X_ratios = data_ratios[ratios +[\"Result\"]]\n",
    "y_ratios = data_ratios[target]\n",
    "\n",
    "# Normalize the features\n",
    "X_scaled_ratios = scaler.fit_transform(X_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2mL = pd.read_csv('../data/2mL.csv' , sep='\\t', index_col=0, header=0)\n",
    "df_2mL = utils.normalize_column_names(df_2mL)\n",
    "# Ensure numeric columns are treated as such\n",
    "cols_2mL_to_num = ['TIMP-2', 'ADIPOQ', 'MMP9', \n",
    "                    'KPYM', 'AGRIN', 'PERM', 'HSPB1',\n",
    "                    'Total_protein_BCA']\n",
    "df_2mL = utils.cols_as_numbers(df_2mL, cols_2mL_to_num)\n",
    "# Ensure categorical columns are treated as such\n",
    "df_2mL = utils.cols_as_category(df_2mL, {'Pathology':{\n",
    "                                            'Benigna': 0, \n",
    "                                            'Adenocarcinoma de endometrio': 1,\n",
    "                                            'Otros': np.nan,\n",
    "                                            'Hiperplasia atípica endometrial': np.nan,\n",
    "                                        }})\n",
    "\n",
    "# Columns to be considered as biomarkers\n",
    "BIOMARKERS_2mL = ['TIMP-2', 'ADIPOQ', 'MMP9', 'KPYM', 'AGRIN', 'PERM', 'HSPB1']\n",
    "# Create new columns with the ratios between the biomarkers\n",
    "for biomarker1 in BIOMARKERS_2mL:\n",
    "    for biomarker2 in BIOMARKERS_2mL:\n",
    "        if biomarker1 != biomarker2:\n",
    "            df_2mL[f'{biomarker1}_{biomarker2}'] = df_2mL[biomarker1].div(df_2mL[biomarker2], axis=0)\n",
    "            # Make infinite values NaN\n",
    "            df_2mL[f'{biomarker1}_{biomarker2}'] = df_2mL[f'{biomarker1}_{biomarker2}'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "RATIOS_2mL = [f'{biomarker1}_{biomarker2}' for biomarker1 in BIOMARKERS_2mL for biomarker2 in BIOMARKERS_2mL if biomarker1 != biomarker2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2mL['Result'] = df_2mL.index.map(df_pipelle['Result'])\n",
    "df_2mL['Result'] = df_2mL['Result'].replace({\"Material Insuficiente o ausencia de diagnóstico\": 1, \"Cáncer\": 2})\n",
    "df_2mL['Result'] = df_2mL['Result'].fillna(1)\n",
    "# Now all the remaining string values are replaced by 4\n",
    "df_2mL['Result'] = df_2mL['Result'].replace({value: 0 for value in df_2mL['Result'].unique() if type(value) == str})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMP9 ng/mL', 'HSPB1 ng/mL', 'PERM ng/mL', 'ADIPOQ ng/mL',\n",
       "       'TIMP-2 ng/mL', 'AGRIN ng/mL', 'KPYM ng/mL', 'Result', 'Pathology',\n",
       "       'MMP9 ng/mL_to_AGRIN ng/mL', 'MMP9 ng/mL_to_TIMP-2 ng/mL',\n",
       "       'MMP9 ng/mL_to_PERM ng/mL', 'MMP9 ng/mL_to_KPYM ng/mL',\n",
       "       'MMP9 ng/mL_to_ADIPOQ ng/mL', 'MMP9 ng/mL_to_HSPB1 ng/mL',\n",
       "       'AGRIN ng/mL_to_TIMP-2 ng/mL', 'AGRIN ng/mL_to_PERM ng/mL',\n",
       "       'AGRIN ng/mL_to_KPYM ng/mL', 'AGRIN ng/mL_to_ADIPOQ ng/mL',\n",
       "       'AGRIN ng/mL_to_HSPB1 ng/mL', 'TIMP-2 ng/mL_to_PERM ng/mL',\n",
       "       'TIMP-2 ng/mL_to_KPYM ng/mL', 'TIMP-2 ng/mL_to_ADIPOQ ng/mL',\n",
       "       'TIMP-2 ng/mL_to_HSPB1 ng/mL', 'PERM ng/mL_to_KPYM ng/mL',\n",
       "       'PERM ng/mL_to_ADIPOQ ng/mL', 'PERM ng/mL_to_HSPB1 ng/mL',\n",
       "       'KPYM ng/mL_to_ADIPOQ ng/mL', 'KPYM ng/mL_to_HSPB1 ng/mL',\n",
       "       'ADIPOQ ng/mL_to_HSPB1 ng/mL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \" ng/mL\" with \"\" in the column names\n",
    "data_clean.columns = data_clean.columns.str.replace(\" ng/mL\", \"\")\n",
    "data_clean.columns = data_clean.columns.str.replace(\"_to_\", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TIMP-2', 'ADIPOQ', 'MMP9', 'KPYM', 'AGRIN', 'PERM', 'HSPB1',\n",
       "       'Total_protein_BCA', 'Pathology', 'TIMP-2_ADIPOQ', 'TIMP-2_MMP9',\n",
       "       'TIMP-2_KPYM', 'TIMP-2_AGRIN', 'TIMP-2_PERM', 'TIMP-2_HSPB1',\n",
       "       'ADIPOQ_TIMP-2', 'ADIPOQ_MMP9', 'ADIPOQ_KPYM', 'ADIPOQ_AGRIN',\n",
       "       'ADIPOQ_PERM', 'ADIPOQ_HSPB1', 'MMP9_TIMP-2', 'MMP9_ADIPOQ',\n",
       "       'MMP9_KPYM', 'MMP9_AGRIN', 'MMP9_PERM', 'MMP9_HSPB1', 'KPYM_TIMP-2',\n",
       "       'KPYM_ADIPOQ', 'KPYM_MMP9', 'KPYM_AGRIN', 'KPYM_PERM', 'KPYM_HSPB1',\n",
       "       'AGRIN_TIMP-2', 'AGRIN_ADIPOQ', 'AGRIN_MMP9', 'AGRIN_KPYM',\n",
       "       'AGRIN_PERM', 'AGRIN_HSPB1', 'PERM_TIMP-2', 'PERM_ADIPOQ', 'PERM_MMP9',\n",
       "       'PERM_KPYM', 'PERM_AGRIN', 'PERM_HSPB1', 'HSPB1_TIMP-2', 'HSPB1_ADIPOQ',\n",
       "       'HSPB1_MMP9', 'HSPB1_KPYM', 'HSPB1_AGRIN', 'HSPB1_PERM', 'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2mL.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2e54e4b920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "AUC with AGRIN_TIMP-2: 0.9704433497536945\n"
     ]
    }
   ],
   "source": [
    "# Build a model using \"AGRIN_TIMP-2\" and \"Result\" as features\n",
    "# Train it over df_mmk and evaluate it over df_2mL\n",
    "features = ['AGRIN_TIMP-2', 'Result']\n",
    "target = 'Pathology'\n",
    "\n",
    "# Preprocess data\n",
    "df_2mL[features] = df_2mL[features].apply(pd.to_numeric, errors='coerce')\n",
    "data_clean[features] = data_clean[features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df_2mL_clean = df_2mL[features + [target]].dropna()\n",
    "df_mmk_clean = data_clean[features + [target]].dropna()\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df_2mL_clean[target] = label_encoder.fit_transform(df_2mL_clean[target])\n",
    "df_mmk_clean[target] = label_encoder.fit_transform(df_mmk_clean[target])\n",
    "\n",
    "# Split into features and target\n",
    "X_train = df_mmk_clean[features]\n",
    "y_train = df_mmk_clean[target]\n",
    "X_test = df_2mL_clean[features]\n",
    "y_test = df_2mL_clean[target]\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Build the neural network model using the build_model function\n",
    "model = build_model(input_dim=X_train.shape[1], nodes=[64, 32], dropout_rate=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = fit_model(model, X_train, y_train, X_test, y_test, epochs=200, batch_size=16)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"AUC with AGRIN_TIMP-2: {auc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>NPV</th>\n",
       "      <th>PPV</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TNR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FNR</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.707</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.051</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.524</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.744</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.524</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.212</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.763</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.232</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.619</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.619</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.242</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.806</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.253</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.848</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.875</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.354</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.900</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.923</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.952</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Sensitivity  Specificity    NPV    PPV  TN  FP  FN  TP    TNR  \\\n",
       "3       0.030        1.000        0.000    NaN  0.580   0  21   0  29  0.000   \n",
       "4       0.040        1.000        0.429  1.000  0.707   9  12   0  29  0.429   \n",
       "5       0.051        1.000        0.524  1.000  0.744  11  10   0  29  0.524   \n",
       "21      0.212        1.000        0.571  1.000  0.763  12   9   0  29  0.571   \n",
       "23      0.232        1.000        0.619  1.000  0.784  13   8   0  29  0.619   \n",
       "24      0.242        1.000        0.667  1.000  0.806  14   7   0  29  0.667   \n",
       "25      0.253        0.966        0.762  0.941  0.848  16   5   1  28  0.762   \n",
       "26      0.263        0.966        0.810  0.944  0.875  17   4   1  28  0.810   \n",
       "35      0.354        0.931        0.857  0.900  0.900  18   3   2  27  0.857   \n",
       "49      0.495        0.828        0.905  0.792  0.923  19   2   5  24  0.905   \n",
       "67      0.677        0.690        0.952  0.690  0.952  20   1   9  20  0.952   \n",
       "99      1.000        0.000        1.000  0.420    NaN  21   0  29   0  1.000   \n",
       "\n",
       "      TPR    FNR    FPR  \n",
       "3   1.000  0.000  1.000  \n",
       "4   1.000  0.000  0.571  \n",
       "5   1.000  0.000  0.476  \n",
       "21  1.000  0.000  0.429  \n",
       "23  1.000  0.000  0.381  \n",
       "24  1.000  0.000  0.333  \n",
       "25  0.966  0.034  0.238  \n",
       "26  0.966  0.034  0.190  \n",
       "35  0.931  0.069  0.143  \n",
       "49  0.828  0.172  0.095  \n",
       "67  0.690  0.310  0.048  \n",
       "99  0.000  1.000  0.000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a df of the sensitivity, specificity, NPV, PPV, TN, FP, FN, TP, TNR, TPR, FNR, FPR\n",
    "# for all the possible thresholds\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "npv = []\n",
    "ppv = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "tp = []\n",
    "tnr = []\n",
    "tpr = []\n",
    "fnr = []\n",
    "fpr = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresholded = y_pred_proba > threshold\n",
    "    tn_, fp_, fn_, tp_ = confusion_matrix(y_test, y_pred_thresholded).ravel()\n",
    "    tn.append(tn_)\n",
    "    fp.append(fp_)\n",
    "    fn.append(fn_)\n",
    "    tp.append(tp_)\n",
    "    sensitivity.append(tp_ / (tp_ + fn_))\n",
    "    specificity.append(tn_ / (tn_ + fp_))\n",
    "    npv.append(tn_ / (tn_ + fn_))\n",
    "    ppv.append(tp_ / (tp_ + fp_))\n",
    "    tnr.append(tn_ / (tn_ + fp_))\n",
    "    tpr.append(tp_ / (tp_ + fn_))\n",
    "    fnr.append(fn_ / (fn_ + tp_))\n",
    "    fpr.append(fp_ / (fp_ + tn_))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Threshold': thresholds,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'NPV': npv,\n",
    "    'PPV': ppv,\n",
    "    'TN': tn,\n",
    "    'FP': fp,\n",
    "    'FN': fn,\n",
    "    'TP': tp,\n",
    "    'TNR': tnr,\n",
    "    'TPR': tpr,\n",
    "    'FNR': fnr,\n",
    "    'FPR': fpr\n",
    "})\n",
    "\n",
    "# Remove repeated sensitivity values taking the last\n",
    "df_metrics = df_metrics.drop_duplicates(subset=['Specificity'], keep='last')\n",
    "\n",
    "# Round to 3 decimal places\n",
    "df_metrics = df_metrics.round(3)\n",
    "\n",
    "df_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result  Pathology\n",
       "2       1.0          16\n",
       "1       1.0          14\n",
       "0       0.0          13\n",
       "1       0.0           9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2mL[[\"Result\",\"Pathology\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
