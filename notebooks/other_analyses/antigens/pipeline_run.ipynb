{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-6.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m/home/leandro/Dropbox/workspacesbg/sbg/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../../../code/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01morf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mOrf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OnlineOrf\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAlphaFoldHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlphaFoldHandler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mStructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/workspacesbg/sbg/sbg/orf/Orf.py:11\u001b[39m\n\u001b[32m      8\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m/home/leandro/Dropbox/workspacesbg/sbg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mURLRetrieveHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m URLRetrieveHandler\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyxml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mXMLToPy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XML2Py\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mFileHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/workspacesbg/sbg/sbg/pyxml/XMLToPy.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mXML2Py - XML to Python de-serialization\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    print python_object\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m etree\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mXML2Py\u001b[39;00m():\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m( \u001b[38;5;28mself\u001b[39m ):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "# Import the sbg workspace and local utils\n",
    "import sys\n",
    "sys.path.append('/home/leandro/Dropbox/workspacesbg/sbg/')\n",
    "sys.path.append('../../../code/')\n",
    "\n",
    "from sbg.orf.Orf import OnlineOrf\n",
    "from sbg.structure.AlphaFoldHandler import AlphaFoldHandler\n",
    "from sbg.structure.Structure import Structure\n",
    "from sbg.common.FileHandler import FileHandler\n",
    "from sbg.scripts.foldx.TangoHandler import TangoHandler\n",
    "\n",
    "import pipeline_functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Bio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msequence_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m highlight_sequence, align_sequences_biopython, display_alignment_with_highlighting\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Insync/leandro.radusky@gmail.com/Google Drive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/../../../code/sequence_utils.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlignIO\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAlign\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultipleSeqAlignment\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSeqRecord\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeqRecord\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Bio'"
     ]
    }
   ],
   "source": [
    "# External libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sequence_utils import highlight_sequence, align_sequences_biopython, display_alignment_with_highlighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from data/protein_data.csv\n",
    "# data = pd.read_csv('data/protein_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for Irene\n",
    "#up_accs = ['P05067','P02452','O00468','Q9UL52','P10451','P35321','A8K2U0','P22528','P50454','P04275','P16035','P80188']\n",
    "#up_accs = ['Q9UL52']\n",
    "up_accs = ['O60504', 'P17252', 'P07585', 'O94788', 'O15247', 'P84157']\n",
    "# create a data dataframe\n",
    "data = pd.DataFrame(up_accs, columns=['up_acc'])\n",
    "data = data.set_index('up_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Q9UL52 \n",
      "[2025-04-09 11:00:12,332] Single-chain mode set, will predict PDBs as single chains\n",
      "[2025-04-09 11:00:12,333] Single PDB file input (Q9UL52), extracting single chains to /tmp/tmpwt9psvh3\n",
      "[2025-04-09 11:00:12,393] Found 1 extracted single chains in /tmp/tmpwt9psvh3\n",
      "[2025-04-09 11:00:12,393] Pre-processing PDBs using ESM-IF1\n",
      "[2025-04-09 11:00:23,295] Predicting PDBs with DiscoTope-3.0\n",
      "[2025-04-09 11:00:24,426] Saving predictions for Q9UL52_A.pdb to /home/leandro/Insync/gdrive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/data/results_screenEC/Q9/Q9UL52//Q9UL52/Q9UL52_A_discotope3.csv\n",
      "[2025-04-09 11:00:24,509] Predicted 1 / 1 PDB files extracted from 1 input PDBs, saved to /home/leandro/Insync/gdrive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/data/results_screenEC/Q9/Q9UL52//Q9UL52\n"
     ]
    }
   ],
   "source": [
    "entry = 0\n",
    "for up_acc in data.index:\n",
    "    try:\n",
    "        entry += 1\n",
    "        print(f'Processing {up_acc} ') #{data.loc[up_acc, 'swiss_prot']}, {entry} of {len(data.index)}')\n",
    "\n",
    "        results_dir = f'/home/leandro/Insync/gdrive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/data/results_KPYM_combined_BMKs/{up_acc[0:2]}/{up_acc}/'\n",
    "        FileHandler.ensureDir(results_dir)\n",
    "\n",
    "        # Step 1: Get the Uniprot object\n",
    "        step = 1\n",
    "        uniprot_object = OnlineOrf(up_acc)\n",
    "        step = 1.1\n",
    "        glycosilation_sites = uniprot_object.getGlycosylationSites()\n",
    "        if len(uniprot_object.getGlycosylationSites()) > 0:\n",
    "            glycosylation_df = pd.DataFrame(uniprot_object.getGlycosylationSites(), columns=['res', 'glycosylation'])\n",
    "            glycosylation_df.to_csv(results_dir + f'{up_acc}_glycosylation.csv')\n",
    "        \n",
    "        step = 1.2\n",
    "        modified_residues = uniprot_object.getModifiedResidues()\n",
    "        if len(uniprot_object.getModifiedResidues()) > 0:\n",
    "            modified_residues_df = pd.DataFrame(uniprot_object.getModifiedResidues(), columns=['res', 'modification'])\n",
    "            modified_residues_df.to_csv(results_dir + f'{up_acc}_modifications.csv')\n",
    "        \n",
    "        # step = 1.3\n",
    "        try:\n",
    "            isoforms = uniprot_object.getIsoforms()\n",
    "        except:\n",
    "            isoforms = []\n",
    "        step = 1.4\n",
    "        uniprot_string_id = uniprot_object.getStringId()\n",
    "        step = 1.5\n",
    "        try:\n",
    "            subcellular_location = uniprot_object.getSubcellularLocation()\n",
    "        except:\n",
    "            subcellular_location = None\n",
    "        step=1.6\n",
    "        crytals = uniprot_object.getCrystals()\n",
    "\n",
    "        # Step 2: Get the AlphaFold object\n",
    "        step = 2\n",
    "        alphafold_object = AlphaFoldHandler().get_model(up_acc)\n",
    "\n",
    "        # Step 3: Run discotope 3.0 and save the results, then load them\n",
    "        step = 3\n",
    "        if alphafold_object is not None:\n",
    "            discotope3_file = f'{up_acc}_A_discotope3.csv'\n",
    "            # If no results, run discotope\n",
    "            if not FileHandler.fileExists(path=results_dir + discotope3_file):\n",
    "                pf.run_discotope(up_acc, save_to=results_dir)\n",
    "            \n",
    "            # Now, if the results are there, load them\n",
    "            if FileHandler.fileExists(path=results_dir + discotope3_file):\n",
    "                discotope_results = pd.read_csv(results_dir + discotope3_file, index_col=0)\n",
    "            else:\n",
    "                discotope_results = None\n",
    "\n",
    "        # Step 4: Compute the expression \n",
    "        step = 4\n",
    "        exp_file = f'{up_acc}_exp_results.csv'\n",
    "        #if FileHandler.fileExists(results_dir + exp_file):\n",
    "        #    exp_results = pd.read_csv(results_dir + exp_file, index_col=0)\n",
    "        #else:\n",
    "        try:\n",
    "            exp_auc, exp_normal, exp_cancer = pf.compare_gene_expression(gene_name=uniprot_object.getGeneName(), up_acc=up_acc, save_to=results_dir)\n",
    "        except:\n",
    "            exp_auc, exp_normal, exp_cancer = None, None, None\n",
    "        \n",
    "        exp_results = pd.DataFrame({'exp_auc': [exp_auc], 'exp_normal': [exp_normal], 'exp_cancer': [exp_cancer]})\n",
    "        exp_results.to_csv(results_dir + exp_file)\n",
    "\n",
    "        # Step 5: Align the isoforms\n",
    "        step = 5\n",
    "        if not FileHandler.fileExists(results_dir + f'{up_acc}_isoforms_alignment.html') and len(isoforms) > 1:\n",
    "            secuences = []\n",
    "            names = []\n",
    "            for isoform in isoforms:\n",
    "                isoform_obj = OnlineOrf(isoform)\n",
    "                secuences.append(isoform_obj.getSequence())\n",
    "                names.append(isoform)\n",
    "            alignment = align_sequences_biopython(secuences, names)\n",
    "            display_alignment_with_highlighting(alignment, save_to=results_dir + f'{up_acc}_isoforms_alignment.html')\n",
    "\n",
    "        # Step 6: Compute the aggregation propensity\n",
    "        step = 6\n",
    "        if alphafold_object is not None:\n",
    "            if not FileHandler.fileExists(results_dir + f'{up_acc}_agg.txt'):\n",
    "                TangoHandler.getAggregation(alphafold_object,results_dir)\n",
    "\n",
    "            agg_df = pd.read_csv(results_dir + f'{up_acc}_agg.txt', sep='\\t', header=0, index_col=\"res\")[[\"Aggregation\"]]\n",
    "        \n",
    "        # Step 7: Get interaction partners\n",
    "        step = 7\n",
    "        if not FileHandler.fileExists(results_dir + f'{up_acc}_interactors.tsv'):\n",
    "            interactors = pf.get_interactors(uniprot_string_id, up_acc, save_to=results_dir)\n",
    "        else:\n",
    "            interactors = pd.read_csv(results_dir + f'{up_acc}_interactors.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "        # Step 8: Get the protein homology\n",
    "        step = 8\n",
    "        if not FileHandler.fileExists(results_dir + f'{up_acc}_homologs.tsv'):\n",
    "            homology = pf.get_homologs(uniprot_object.getGeneName(), up_acc, save_to=results_dir)\n",
    "        else:\n",
    "            homology = pd.read_csv(results_dir + f'{up_acc}_homologs.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "        # Step 9: Get the protein structures bioassemblies to check for homo and multimers\n",
    "        step = 9\n",
    "        if not FileHandler.fileExists(results_dir + f'{up_acc}_bioassemblies.csv'):\n",
    "            try:\n",
    "                all_dataframes = []\n",
    "                for crystal in crytals:\n",
    "                    structure = Structure(crystal, replaceExistent=False)\n",
    "                    df = structure.analyzeStructureType()\n",
    "                    df['crystal_id'] = crystal\n",
    "                    all_dataframes.append(df)\n",
    "\n",
    "                if len(all_dataframes) > 1:\n",
    "                    bioassemblies_df = pd.concat(all_dataframes)\n",
    "                elif len(all_dataframes) == 1:\n",
    "                    bioassemblies_df = all_dataframes[0]\n",
    "                else:\n",
    "                    bioassemblies_df = None\n",
    "\n",
    "                if len(all_dataframes) > 0:\n",
    "                    bioassemblies_df.to_csv(results_dir+f'{up_acc}_bioassemblies.csv')\n",
    "            except:\n",
    "                bioassemblies_df = None\n",
    "        else:\n",
    "            bioassemblies_df = pd.read_csv(results_dir + f'{up_acc}_bioassemblies.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error with {up_acc} in step {step}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary: epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_accs = {\n",
    "    \"KPYM\": \"P14618\",\n",
    "    \"MMP9\": \"P14780\",\n",
    "    \"HSPB1\": \"P04792\",\n",
    "    \"AGRIN\": \"O00468\",\n",
    "    \"CLIC1\": \"O00299\"\n",
    "}\n",
    "\n",
    "epitopes = {\n",
    "    \"KPYM\": [\"EAVRMQHLIARE\"],\n",
    "    \"MMP9\": [\"TFLGKEY\", \"GYPFDGKD\"],\n",
    "    \"HSPB1\": [\"KDGVV\"],\n",
    "    \"AGRIN\": [\"RLELSRHW\", \"FVGAGLRGC\", \"NPCHGAAPC\", \"RDRRLEF\", \"GHPCLNGASC\", \"VCLCPGGF\"],\n",
    "    \"CLIC1\": [\"KRRTE\", \"QVELF\", \"KRRTET\"] \n",
    "}\n",
    "\n",
    "\n",
    "up_objs = {k: OnlineOrf(v) for k, v in up_accs.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KPYM\n",
      "Epitope: EAVRMQHLIARE\n",
      "Positions: [(372, 384)]\n",
      "\n",
      "\n",
      "\n",
      "Processing MMP9\n",
      "Epitope: TFLGKEY\n",
      "Positions: [(351, 358)]\n",
      "Epitope: GYPFDGKD\n",
      "Positions: [(177, 185)]\n",
      "\n",
      "\n",
      "\n",
      "Processing HSPB1\n",
      "Epitope: KDGVV\n",
      "Positions: [(113, 118)]\n",
      "\n",
      "\n",
      "\n",
      "Processing AGRIN\n",
      "Epitope: RLELSRHW\n",
      "Positions: [(1455, 1463)]\n",
      "Epitope: FVGAGLRGC\n",
      "Positions: [(1510, 1519)]\n",
      "Epitope: NPCHGAAPC\n",
      "Positions: [(1594, 1603)]\n",
      "Epitope: RDRRLEF\n",
      "Positions: [(1695, 1702)]\n",
      "Epitope: GHPCLNGASC\n",
      "Positions: [(1826, 1836)]\n",
      "Epitope: VCLCPGGF\n",
      "Positions: [(1843, 1851)]\n",
      "\n",
      "\n",
      "\n",
      "Processing CLIC1\n",
      "Epitope: KRRTE\n",
      "Positions: [(48, 53)]\n",
      "Epitope: QVELF\n",
      "Positions: [(6, 11)]\n",
      "Epitope: KRRTET\n",
      "Positions: [(48, 54)]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_all_epitopes(seq, epitope):\n",
    "    positions = []\n",
    "    for i in range(len(seq) - len(epitope)):\n",
    "        if seq[i:i+len(epitope)] == epitope:\n",
    "            positions.append((i, i+len(epitope)))\n",
    "    return positions\n",
    "\n",
    "for up in up_objs.keys():\n",
    "    print(f'Processing {up}')\n",
    "    up_obj = up_objs[up]\n",
    "    seq = up_obj.getSequence()\n",
    "\n",
    "    for epitope in epitopes[up]:\n",
    "        print(f'Epitope: {epitope}')\n",
    "        print(f'Positions: {find_all_epitopes(seq, epitope)}')\n",
    "\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
