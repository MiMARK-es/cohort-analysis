{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sbg workspace and local utils\n",
    "# Import the sbg workspace and local utils\n",
    "import sys\n",
    "sys.path.append('/home/leandro/Dropbox/workspacesbg/sbg/')\n",
    "sys.path.append('../../../code/')\n",
    "\n",
    "from sbg.orf.Orf import OnlineOrf\n",
    "import pipeline_functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from data/protein_data.csv\n",
    "# data = pd.read_csv('data/protein_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for Irene\n",
    "up_accs = ['P05067','P02452','O00468','Q5S248','P10451','P35321','A8K2U0','P22528','P50454','P04275','P16035','P80188']\n",
    "\n",
    "# create a data dataframe\n",
    "data = pd.DataFrame(up_accs, columns=['up_acc'])\n",
    "data = data.set_index('up_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results file generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dir = \"/home/leandro/Insync/gdrive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/data/results_screenEC/\"\n",
    "summary_data = []  # List to store each protein's summary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Error processing P05067: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P02452: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing O00468: name 'swiss_prot' is not defined\n",
      "Error processing Q5S248: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P10451: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P35321: name 'swiss_prot' is not defined\n",
      "\n",
      "Error processing A8K2U0: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P22528: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P50454: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P04275: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P16035: name 'swiss_prot' is not defined\n",
      "\n",
      "\n",
      "Error processing P80188: name 'swiss_prot' is not defined\n",
      "General summary saved to /home/leandro/Insync/gdrive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/data/results_screenEC/general_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "summary_data = []\n",
    "for up_acc in data.index:\n",
    "    try:\n",
    "        step += 1\n",
    "        # Define result directory for this entry\n",
    "        results_dir = f'{results_base_dir}/{up_acc[0:2]}/{up_acc}/'\n",
    "\n",
    "        # For Irene, we will use the following fields\n",
    "        oorf = OnlineOrf(up_acc)\n",
    "        gene_name = oorf.getGeneName()\n",
    "        protein_name = oorf.getName()\n",
    "        swiss_prot = \"NA\"\n",
    "        up_down_regulated = \"NA\"\n",
    "        family = \"NA\"\n",
    "        biomarker_application = \"NA\"\n",
    "\n",
    "        # For the original MS data, we have used these:\n",
    "        # Retrieve relevant fields from the original data\n",
    "        # gene_name = data.loc[up_acc, 'gene_name']\n",
    "        # protein_name = data.loc[up_acc, 'protein_name']\n",
    "        # swiss_prot = data.loc[up_acc, 'swiss_prot']\n",
    "        # up_down_regulated = data.loc[up_acc, 'up_down_regulated']\n",
    "        # family = data.loc[up_acc, 'family']\n",
    "        # biomarker_application = data.loc[up_acc, 'biomarker_application']\n",
    "        \n",
    "        # Step 1: Get the Uniprot object (like in the original pipeline)\n",
    "        try:\n",
    "            uniprot_object = OnlineOrf(up_acc)\n",
    "        except:\n",
    "            print(f\"Error retrieving {up_acc}\")\n",
    "            continue\n",
    "        \n",
    "        # Quick computations from OnlineOrf object\n",
    "        glycosylation_sites = uniprot_object.getGlycosylationSites()\n",
    "        modified_residues = uniprot_object.getModifiedResidues()\n",
    "        try:\n",
    "            subcellular_location = \" \\\\| \".join(uniprot_object.getSubcellularLocation()) if uniprot_object.getSubcellularLocation() else 'N/A'\n",
    "        except:\n",
    "            subcellular_location = 'N/A'\n",
    "        try:\n",
    "            isoforms = uniprot_object.getIsoforms()\n",
    "        except:\n",
    "            isoforms = []\n",
    "        number_of_isoforms = len(isoforms) if isoforms else 0\n",
    "        \n",
    "        # Load data files\n",
    "        discotope_results = pd.read_csv(f'{results_dir}{up_acc}_A_discotope3.csv') if os.path.exists(f'{results_dir}{up_acc}_A_discotope3.csv') else None\n",
    "        agg_data = pd.read_csv(f'{results_dir}{up_acc}_agg.txt', sep='\\t') if os.path.exists(f'{results_dir}{up_acc}_agg.txt') else None\n",
    "        exp_results = pd.read_csv(f'{results_dir}{up_acc}_exp_results.csv') if os.path.exists(f'{results_dir}{up_acc}_exp_results.csv') else None\n",
    "        interactors = pd.read_csv(f'{results_dir}{up_acc}_interactors.tsv', sep='\\t') if os.path.exists(f'{results_dir}{up_acc}_interactors.tsv') else None\n",
    "        homologs = pd.read_csv(f'{results_dir}{up_acc}_homologs.tsv', sep='\\t') if os.path.exists(f'{results_dir}{up_acc}_homologs.tsv') else None\n",
    "        bioassemblies = pd.read_csv(f'{results_dir}{up_acc}_bioassemblies.csv') if os.path.exists(f'{results_dir}{up_acc}_bioassemblies.csv') else None\n",
    "        \n",
    "        # Retrieve important metrics for summary\n",
    "        exp_auc, fold_change, up_down_regulated_transcriptomics = pf.get_expression_metrics(exp_results)\n",
    "        epitope_count = pf.count_discotope_epitopes(discotope_results)\n",
    "        agg_critical_residues = pf.count_critical_aggregation_sites(agg_data)\n",
    "        homo_max_n_uniprots, hetero_max_n_uniprots = pf.get_bioassemblies_metrics(bioassemblies)\n",
    "\n",
    "        # Create a unified DataFrame per residue\n",
    "        residues_df = pf.create_unified_residue_df(discotope_results, agg_data, glycosylation_sites, modified_residues)\n",
    "        \n",
    "        # Format interactors and homologs for the summary table\n",
    "        interactor_count, formatted_interactors = pf.format_interactors_homologs(interactors, \"preferredName_B\")\n",
    "        homolog_count, formatted_homologs = pf.format_interactors_homologs(homologs, \"gene_id\")\n",
    "\n",
    "        # Compile metrics for the summary\n",
    "        metrics = {\n",
    "            'AUC': exp_auc,\n",
    "            'Fold Change': fold_change,\n",
    "            'Up/Down Regulated' : up_down_regulated_transcriptomics,\n",
    "            'Discotope Epitope Count': epitope_count,\n",
    "            'Max n_uniprots Homo': homo_max_n_uniprots,\n",
    "            'Max n_uniprots Hetero': hetero_max_n_uniprots\n",
    "        }\n",
    "        \n",
    "        # Append data to the summary list\n",
    "        summary_data.append({\n",
    "            'Uniprot ID': up_acc,\n",
    "            'Gene Name': gene_name,\n",
    "            'Protein Name': protein_name,\n",
    "            'Swiss Prot': swiss_prot,\n",
    "            'Up/Down Regulated': up_down_regulated,\n",
    "            'Family': family,\n",
    "            'Biomarker Application': biomarker_application,\n",
    "            '(transcriptomics) AUC': exp_auc,\n",
    "            '(transcriptomics) Fold Change ': fold_change,\n",
    "            '(transcriptomics) Up/Down Regulated ': up_down_regulated_transcriptomics,\n",
    "            'Seq Length': len(uniprot_object.getSequence()),\n",
    "            'Glycosylation Sites': len(glycosylation_sites),\n",
    "            'Modified Residues': len(modified_residues),\n",
    "            'Subcellular Location': subcellular_location,\n",
    "            'Discotope Epitope Count': epitope_count,\n",
    "            'Critical Aggregation Sites (>50)': agg_critical_residues,\n",
    "            'Interactor Count': interactor_count,\n",
    "            'Interactors': formatted_interactors,\n",
    "            'Homolog Count': homolog_count,\n",
    "            'Homologs': formatted_homologs,\n",
    "            'Max n_uniprots Homo': homo_max_n_uniprots,\n",
    "            'Max n_uniprots Hetero': hetero_max_n_uniprots,\n",
    "            'Number of Isoforms': number_of_isoforms\n",
    "        })\n",
    "        \n",
    "        # Save detailed markdown file\n",
    "        pf.save_detailed_markdown(\n",
    "            up_acc, results_dir, residues_df, metrics, exp_results, \n",
    "            glycosylation_sites, modified_residues, interactors, homologs, bioassemblies, {\n",
    "                'gene_name': gene_name,\n",
    "                'protein_name': protein_name,\n",
    "                'swiss_prot': swiss_prot,\n",
    "                'up_down_regulated': up_down_regulated,\n",
    "                'family': family,\n",
    "                'biomarker_application': biomarker_application\n",
    "            },\n",
    "            number_of_isoforms\n",
    "        )\n",
    "        \n",
    "        print(f'Processed {up_acc}, {step} out of {len(data.index)}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error processing {up_acc}: {e}')\n",
    "\n",
    "# Create a DataFrame from the collected summary data\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save the summary DataFrame to Excel\n",
    "excel_file_path = os.path.join(results_base_dir, \"general_summary.xlsx\")\n",
    "summary_df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"General summary saved to {excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_dataframe(df):\n",
    "    # Iterate over each column to handle mixed or complex types\n",
    "    for column in df.columns:\n",
    "        def flatten_value(value):\n",
    "            # If the value is a pandas Series, extract the first element or convert to string\n",
    "            if isinstance(value, pd.Series):\n",
    "                return str(value.iloc[0]) if not value.empty else 'N/A'\n",
    "            # Convert None or NaN to 'N/A'\n",
    "            elif pd.isna(value):\n",
    "                return 'N/A'\n",
    "            # Convert other types to string\n",
    "            else:\n",
    "                return str(value)\n",
    "        \n",
    "        # Apply the flattening to each column\n",
    "        df[column] = df[column].apply(flatten_value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Sanitize the summary_df\n",
    "summary_df = sanitize_dataframe(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Uniprot ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the general markdown file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_general_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_base_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Insync/leandro.radusky@gmail.com/Google Drive/Mimark/code/cohort-analysis/notebooks/other_analyses/antigens/pipeline_functions.py:366\u001b[0m, in \u001b[0;36msave_general_markdown\u001b[0;34m(summary_df, base_dir)\u001b[0m\n\u001b[1;32m    363\u001b[0m general_md_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneral_summary.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Modify the Uniprot ID column to include Markdown links directly\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m summary_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniprot ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msummary_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUniprot ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m](./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_details.md)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(general_md_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m general_file:\n\u001b[1;32m    371\u001b[0m     general_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# General Summary of Protein Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Insync/leandro.radusky@gmail.com/Google Drive/Mimark/code/cohort-analysis/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Insync/leandro.radusky@gmail.com/Google Drive/Mimark/code/cohort-analysis/.venv/lib/python3.13/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Uniprot ID'"
     ]
    }
   ],
   "source": [
    "# Save the general markdown file\n",
    "pf.save_general_markdown(summary_df, results_base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
